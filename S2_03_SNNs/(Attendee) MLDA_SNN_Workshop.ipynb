{"cells":[{"cell_type":"markdown","metadata":{"id":"uh5e7VhnpEw4"},"source":["# Introduction to Spiking Neural Networks\n","A Machine Learning & Data Analytics Lab (MLDA@EEE) Workshop\\\n","Author: Leow Cong Sheng & Ng Ho Chi\\\n","28 Feb 2022"]},{"cell_type":"markdown","metadata":{"id":"WuM9qbi-ruUF"},"source":["### Section I: Preparation\n","Dependencies required are being imported, and data is being prepared.\n","\n","Before we do anything, we will prepare the datasets. In order of cells, we are going to:\n","1. Import necessary libraries\n","2. Connect to GPU if available. Remember to connect to GPU if possible!\n","3. Define some variables (batch size and path to dataset)\n","4. Create some transformations for the images, as well as apply the transformations on the MNIST dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AdUxWx9mo7qS"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import utils, nn, optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import datasets, transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yu-_dNDVs4v_"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device in use: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gfcn_lE4spnT"},"outputs":[],"source":["# User-specified batch size and data for MNIST\n","batch_size = 128\n","data_path='/data/mnist'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypLkJmjRtG3i"},"outputs":[],"source":["# Define transformations\n","transform = transforms.Compose([\n","            transforms.Resize((28, 28)),\n","            transforms.Grayscale(),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0,), (1,))])\n","\n","mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n","mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWbZYC-HtJPF"},"outputs":[],"source":["# Create DataLoaders\n","train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nv1gihe5tVHN"},"outputs":[],"source":["sample_idx = 0\n","sample_data, sample_label =  mnist_train[sample_idx]\n","plt.imshow(sample_data[0], cmap=\"gray\")\n","print(f\"Label for sample data: {sample_label}\")\n","print(f\"Size of data: {sample_data.size()}\")"]},{"cell_type":"markdown","metadata":{"id":"BJdeNRsso__R"},"source":["## Section II: Traditional Neural Network\n","A brief look into how an example of conventional neural network is used for the MNIST dataset classification.\n","\n","For people more familiar with tensorflow, `nn.Linear` is basically kind of the same as the dense layer there.\n","\n","Convolutional layers are initialized like this: `nn.Conv2d(input_channel, output_channel, kernel_size, stride, padding)`\n","\n","To refresh, in `__init__`, we first create layers without initializing them. Then, in `forward`, we want propagate the function value `x` through all of the layers we've created.\n","\n","Note that the number of input channels of one layer is basically the number of output channels of the previous layer."]},{"cell_type":"markdown","source":["### Definition of Network"],"metadata":{"id":"pU68HT_OVj_6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"162bJbE6uWSB"},"outputs":[],"source":["### To be completed in the workshop ###\n","##### We want two convolutional layers. Keep the kernel_size at 5, stride at 1, padding at 2.\n","class CNN_model(nn.Module):\n","    def __init__(self, input_channel=1):\n","        super(CNN_model, self).__init__()\n","        self.conv1 = ### Initialize Convolutional Layer here ###\n","        self.pool = nn.MaxPool2d(2)\n","        self.conv2 = ### Initialize second Convolutional Layer here ###\n","        self.fc1 = nn.Linear(32 * 14 * 14, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = F.relu(self.conv2(x))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = self.fc1(x)\n","        return x\n","\n","model = CNN_model().to(device)\n","print(model)"]},{"cell_type":"markdown","source":["### Training and Testing\n","\n","Here, we initialize the loss function that will be used when we are going to work on our train and test data, an optimizer, as well as the epoch which will be used later."],"metadata":{"id":"OuGlNLszMs8V"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWI2u3vByICN"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","epoch_num = 2"]},{"cell_type":"markdown","source":["Here, we create the functions for the train batch and test batch. \n","\n","For the train batch, we are focusing on trying to improve the model.\n","\n","For the test batch, we are focusing on the evaluation of the model that was trained by the train batch."],"metadata":{"id":"e8-bSv7UVncd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"n78PGJJUyPzo"},"outputs":[],"source":["def train_batch(model_train, training_dl, epochs, print_step = 100):\n","    model_train.train()\n","    size = len(training_dl.dataset)\n","    for epoch in range(epochs):\n","        running_lost, correct = 0.0, 0\n","        for i, (data, target)in enumerate(training_dl):\n","            inputs, labels = data.to(device), target.to(device)\n","            optimizer.zero_grad() # Clear the gradient\n","            outputs = model_train(inputs) # Try the model on the train inputs\n","            loss = loss_fn(outputs, labels) # Calculate the loss function value based on the outputs from model and the actual values\n","            loss.backward() # Backward propagation\n","            optimizer.step()\n","            correct += (outputs.argmax(1) == labels).float().sum()\n","            if i % print_step == 0:\n","                loss, current = loss.item(), i * len(inputs)\n","                print(f'Epoch: {epoch+1} | loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n","        accuracy = 100 * correct / size\n","        print(f\"Epoch: {epoch+1} | accuracy: {accuracy}\")\n","    print('Finished Training')\n","\n","def test_batch(model_test, testing_dl):\n","    model_test.eval()\n","    size = len(testing_dl.dataset)\n","    test_loss, correct = 0, 0\n","\n","    with torch.no_grad():\n","        for batch, (X, Y) in enumerate(testing_dl):\n","            X, Y = X.to(device), Y.to(device)\n","            pred = model(X)\n","            test_loss += loss_fn(pred, Y).item()\n","            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n","\n","    test_loss /= size\n","    correct = int(correct)\n","\n","   # print(f'Test Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')\n","\n","    print(f\"Total correctly classified test set images: {correct}/{size}\")\n","    print(f\"Test set accuracy: {100 * correct/size:.2f}%, avg loss: {test_loss:>8f}\")"]},{"cell_type":"markdown","source":["### Evaluation\n","Here, we execute the functions created earlier and get the accuracy.\n","\n","We also plot a diagram to compare the first 10 images with the predicted results."],"metadata":{"id":"pls8VrhsMYym"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"k78hn6DG0E60"},"outputs":[],"source":["train_batch(model, train_loader, epoch_num)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVPZrj3R3U9n"},"outputs":[],"source":["test_batch(model, test_loader)"]},{"cell_type":"code","source":["sample = next(iter(test_loader))\n","imgs, lbls = sample\n","imgs, lbls = imgs.to(device), lbls.to(device)\n","actual_number = lbls[:10].cpu().numpy()\n","test_output = model(imgs[:10])\n","y_pred = torch.max(test_output, 1)[1].cpu().data.numpy().squeeze()\n","print(f'Prediction number: {y_pred}')\n","print(f'Actual number: {actual_number}')"],"metadata":{"id":"48ovbi7vLxAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(12,6))\n","for i in range(10):\n","  plt.subplot(3,5, i+1)\n","  plt.tight_layout()\n","  plt.imshow(imgs[i][0].cpu(), cmap='gray', interpolation='none')\n","  plt.title(y_pred[i])\n","  plt.xticks([])\n","  plt.yticks([])"],"metadata":{"id":"ygq1-tbTLyh7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yeNdPoFHsKoL"},"source":["## Section III: Spiking Neural Network using snnTorch\n","The spiking neural network (SNN) will be used instead to draw comparison between the SNN, and the traditional neural network approach in the section above.\\\n","While many different packages have become available in the recent years, we will be implementing a SNN using [snnTorch<sup>1</sup>](https://github.com/jeshraghian/snntorch) in this workshop. The documentation page can be accessed at https://snntorch.readthedocs.io/en/latest/. \\\n","Participants are encouraged to explore other implementation available, with some recommendations listed in the workshop's additional resources.\\\n","<sub><sub>\\[1\\]- [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu “Training Spiking Neural Networks Using Lessons From Deep Learning”. arXiv preprint arXiv:2109.12894, September 2021](https://arxiv.org/abs/2109.12894).</sub></sub>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UK6HZQh4gZB"},"outputs":[],"source":["!pip install snntorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1Br2rwT4elq"},"outputs":[],"source":["import snntorch as snn\n","from snntorch import surrogate\n","import snntorch.functional as SF"]},{"cell_type":"markdown","source":["### Definition of Network\n","The surrogate gradient is solution for the non-differentiable function of the spiking neuron. By smoothing out the spiking function, a differentiable approximation can be obtained, allowing backpropagation through gradient descent to be utilised.\\\n","In this example, the sigmoid function is used to approximate or smooth out the curve, with an argument to specify the slope \\: `surrogate.fast_sigmoid(slope=25)`.\\\n","Additionally, the beta value, and the the number of time steps for the spiking neuron can be specified. The beta value determines how fast the membrane voltage decay expotentially. While this workshop will not cover all the hyperparameters for tuning, you can visit the [LIF documentation page](https://snntorch.readthedocs.io/en/latest/snn.neurons_leaky.html) to see what other parameters can be tuned. For instance, threshold voltage for the neuron is a key parameter which can affect your overall sparsity, and accuracy of the SNN. Number of time steps determine the full time steps for the spiking action.\n"],"metadata":{"id":"wwOiM4awVW3q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aCWZX8Br4y97"},"outputs":[],"source":["### To be completed in the workshop ###\n","beta = 0.9\n","spike_grad = ### Insert surrogate gradient function here ###\n","step_num = 10"]},{"cell_type":"markdown","source":["In the network definition below, we will use a similar CNN architecture we used previously. To convert a typical neural network to a SNN is simple using snnTorch. By using surrogate gradient, backpropagation can be used, and the main changes involve changing the typical neurons in a neural network to be spiking neurons.\\\n","To do so, simply add LIF activations within in between the layers using `snn.Leaky(beta=beta, spike_grad=spike_grad)` with the beta and surrogate gradient function specified previously. These neurons will then be initialized by calling the neurons' `self.lif#.init_leaky()` method in the forward pass.\\\n","The spikes and membrane voltages are produced and stored after each spiking neurons. These will then be used to inject into the subsequent layers and finally be transformed into a output value. To do so, the spikes and membrane can be obtained using `self.lif#(cur#, mem#)`based on the previous current level and membrane voltage."],"metadata":{"id":"pbCh24bq-Tkd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEswOcbxrSrW"},"outputs":[],"source":["### To be completed in the workshop ###\n","class SNN_model(nn.Module):\n","    def __init__(self, num_steps, input_channel=1):\n","        super(SNN_model, self).__init__()\n","        self.num_steps = num_steps\n","        self.conv1 = nn.Conv2d(input_channel, 16, 5, 1, 2)\n","        self.lif1 = ### Insert LIF neuron here ###\n","        self.pool = nn.MaxPool2d(2)\n","        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n","        self.lif2 = ### Insert LIF neuron here ###\n","        self.fc1 = nn.Linear(32*14*14, 10)\n","        self.lif3 = ### Insert LIF neuron here ###\n","\n","    def forward(self, x):\n","        batch_size_curr = x.shape[0] # Get current batch_size\n","        # Initialize hidden states and outputs at t=0\n","        mem1 = ### Initialize the LIF neuron here ###\n","        mem2 = ### Initialize the LIF neuron here ###\n","        mem3 = ### Initialize the LIF neuron here ###\n","\n","        # Record the final layer\n","        spk3_rec = []\n","        mem3_rec = []\n","\n","        for step in range(self.num_steps):\n","            cur1 = self.pool(self.conv1(x))\n","            spk1, mem1 = ### Generate spikes and membrane voltage by using the previous layer's output as injection current ###\n","            cur2 = self.conv2(spk1)\n","            spk2, mem2 = ### Generate spikes and membrane voltage by using the previous layer's output as injection current ###\n","            cur3 = self.fc1(spk2.view(batch_size_curr, -1))\n","            spk3, mem3 = ### Generate spikes and membrane voltage by using the previous layer's output as injection current ###\n","            \n","            spk3_rec.append(spk3)\n","            mem3_rec.append(mem3)\n","\n","        return torch.stack(spk3_rec), torch.stack(mem3_rec)\n","\n","snn_model = SNN_model(step_num).to(device)\n","print(snn_model)"]},{"cell_type":"markdown","source":["To optimize the SNN using backpropagation, the default optimizers, and loss functions available in PyTorch can be used. Other SNN-based optimizers and loss functions are available in snnTorch."],"metadata":{"id":"r8ZAAR5rCSJH"}},{"cell_type":"code","source":["optimizer_snn = torch.optim.Adam(snn_model.parameters(), lr=1e-2, betas=(0.9, 0.999))\n","loss_snn = nn.CrossEntropyLoss()"],"metadata":{"id":"kEvcAC35x3r_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training and Testing"],"metadata":{"id":"BB9KKvlNVd47"}},{"cell_type":"code","source":["epoch_num = 2"],"metadata":{"id":"2XumcZ7aDzuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["*_, last_set = iter(train_loader)\n","data, targets = last_set\n","data = data.to(device)\n","targets = targets.to(device)\n","print(f\"Shape of data: {data.shape}, Shape of targets: {targets.shape}\")"],"metadata":{"id":"HHMqZWzeVuVh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training**\\\n","Similar to the training loop used previously for a traditional neural network, the SNN training loop will involve the calling of the network with the data as the argument `network(data)`. Instead of just returning one variable, this will generate a record of the spikes and the membrane voltages. \\\n","To calculate the loss, the loss function can be called with the membrane voltage record at each time step using `loss_snn(mem_rec[step], targets)`.\\\n","While it is possible to implement the algorithm for converting the spikes record to output and compare with the targets to obtain accuracy, snnTorch offers the function `SF.accuracy_rate(spk_rec, targets)` which allows the accuracy to be obtained.\\\n","**Testing**\\\n","In the testing function, we will now implement the accuracy and loss calculation instead of using the accuracy_rate. Firstly, the predicted values should be obtained by using `test_spk.sum(dim=0).max(1)` which will serves as a way to get the highest value of spike occured (corresponding to the class). Next, instead of calculating the loss at every time step such as in training, we will just calculate the loss which was obtained at the end of the time step using `loss_snn(test_mem[num_steps-1], targets)`."],"metadata":{"id":"XJgAe7BZSM3p"}},{"cell_type":"code","source":["### To be completed in the workshop ###\n","def train_snn(network, train_dl, num_epochs, num_steps):\n","    loss_hist = []\n","    acc_hist = []\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        for i, (data, targets) in enumerate(iter(train_dl)):\n","            data = data.to(device) # Tensor to be accelerated\n","            targets = targets.to(device) # Tensor to be accelerated\n","\n","            network.train() # Set the mode to train\n","            spk_rec, mem_rec = ### Invoke network upon the data ###\n","\n","            loss_val = torch.zeros((1), dtype=torch.float, device=device)\n","            for step in range(num_steps):\n","                loss_val += ### Calculate the loss ###\n","                \n","            # Gradient calculation + weight update\n","            optimizer_snn.zero_grad()\n","            loss_val.backward()\n","            optimizer_snn.step()\n","\n","            # Store loss history for future plotting\n","            loss_hist.append(loss_val.item())\n","\n","            # print every 25 iterations\n","            if i % 450 == 0:\n","                print(f\"Epoch {epoch+1}, Batch {i+1} \\nTrain Loss: {loss_val.item():.2f}\")\n","\n","                # check accuracy on a single batch\n","                acc = ### Use SF's accuracy rate to calculate accuracy ###\n","                acc_hist.append(acc)\n","                print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n","\n","def test_snn(network, test_dl, num_steps):\n","    size, correct = 0, 0\n","    loss_val = 0\n","    with torch.no_grad():\n","        network.eval()\n","        for data, targets in test_dl:\n","            data = data.to(device)\n","            targets = targets.to(device)\n","\n","            test_spk, test_mem = network(data)\n","\n","            _, predicted = ### Obtain the predicted values based on spikes ###\n","            size += targets.size(0)\n","            correct += (predicted==targets).sum().item()\n","            loss_val += ### Update the loss based on the last value of the spiking membrane voltage ###\n","    test_loss = loss_val/size\n","    print(test_loss)\n","    print(f\"Total correctly classified test set images: {correct}/{size}\")\n","    print(f\"Test set accuracy: {100 * correct/size:.2f}%, avg loss: {test_loss:>8f}\")\n"],"metadata":{"id":"rTNkVsR7D1ZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_snn(snn_model, train_loader, epoch_num, step_num)"],"metadata":{"id":"SPQkKJ4yEjUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_snn(snn_model, test_loader, step_num)"],"metadata":{"id":"M-rDn5ICxvW2"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"(Attendee) MLDA_SNN_Workshop.ipynb","provenance":[{"file_id":"15nNXLjueDlE701AREbTk0RgKswwbIvzM","timestamp":1645547507899}],"toc_visible":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}